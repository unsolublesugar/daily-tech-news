"""
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆã®fetch_news.py
æ–°ã—ã„ã‚¯ãƒ©ã‚¹æ§‹é€ ã‚’ä½¿ç”¨ã—ã¦æ—¢å­˜æ©Ÿèƒ½ã‚’å†å®Ÿè£…
"""
import feedparser
import datetime
import os
from pathlib import Path
from xml.dom import minidom
import xml.etree.ElementTree as ET
import requests
from bs4 import BeautifulSoup
import time
import re
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import json
import hashlib

# æ–°ã—ã„ã‚¯ãƒ©ã‚¹ç¾¤ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from archive_config import SiteConfig, PathConfig
from archive_generator import ArchiveGenerator, ArchiveIndexGenerator
from template_manager import TemplateManager

# æ—¢å­˜ã®è¨­å®šã‚’ä¿æŒï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
FEEDS = {
    "Tech Blog Weekly": {
        "url": "https://yamadashy.github.io/tech-blog-rss-feed/feeds/rss.xml",
        "favicon": "ğŸ’»"
    },
    "Zenn": {
        "url": "https://zenn.dev/feed",
        "favicon": "https://zenn.dev/favicon.ico"
    },
    "Qiita": {
        "url": "https://qiita.com/popular-items/feed", 
        "favicon": "https://cdn.qiita.com/assets/favicons/public/production-c620d3e403342b1022967ba5e3db1aaa.ico"
    },
    "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆäººæ°—ï¼‰": {
        "url": "http://b.hatena.ne.jp/hotentry/it.rss",
        "favicon": "https://b.hatena.ne.jp/favicon.ico"
    },
    "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆæ–°ç€ï¼‰": {
        "url": "https://b.hatena.ne.jp/entrylist/it.rss",
        "favicon": "https://b.hatena.ne.jp/favicon.ico"
    },
    "DevelopersIO": {
        "url": "https://dev.classmethod.jp/feed/",
        "favicon": "https://dev.classmethod.jp/favicon.ico"
    },
    "gihyo.jp": {
        "url": "https://gihyo.jp/dev/feed/rss2",
        "favicon": "https://gihyo.jp/favicon.ico"
    },
    "Publickey": {
        "url": "https://www.publickey1.jp/atom.xml",
        "favicon": "https://www.publickey1.jp/favicon.ico"
    },
    "CodeZine": {
        "url": "https://codezine.jp/rss/new/20/index.xml",
        "favicon": "https://codezine.jp/favicon.ico"
    },
    "InfoQ Japan": {
        "url": "https://feed.infoq.com/jp",
        "favicon": "https://www.infoq.com/favicon.ico"
    },
    "connpass - ã‚¤ãƒ™ãƒ³ãƒˆ": {
        "url": "https://connpass.com/explore/ja.atom",
        "favicon": "https://connpass.com/favicon.ico"
    },
    "TECH PLAY - ã‚¤ãƒ™ãƒ³ãƒˆ": {
        "url": "https://rss.techplay.jp/event/w3c-rss-format/rss.xml",
        "favicon": "https://techplay.jp/favicon.ico"
    },
    "O'Reilly Japan - è¿‘åˆŠ": {
        "url": "https://www.oreilly.co.jp/catalog/soon.xml",
        "favicon": "https://www.oreilly.co.jp/favicon.ico"
    }
}

# å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹ã®ä»¶æ•°
MAX_ENTRIES = 5

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
_site_config = SiteConfig()
_path_config = PathConfig()
_archive_generator = ArchiveGenerator(_site_config, _path_config)
_index_generator = ArchiveIndexGenerator(_site_config, _path_config)


# æ—¢å­˜ã®ThumbnailCacheã‚¯ãƒ©ã‚¹ã‚’ãã®ã¾ã¾ä¿æŒ
class ThumbnailCache:
    """ã‚µãƒ ãƒã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_file_path="thumbnail_cache.json"):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        
        Args:
            cache_file_path (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.cache_file_path = cache_file_path
        self.cache = self._load_cache()
    
    def _load_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if os.path.exists(self.cache_file_path):
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Warning: Failed to load thumbnail cache: {e}")
        
        return {}
    
    def _save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
        try:
            with open(self.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Warning: Failed to save thumbnail cache: {e}")
    
    def get(self, url):
        """URLã«å¯¾å¿œã™ã‚‹ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        return self.cache.get(url)
    
    def set(self, url, thumbnail_url):
        """URLã¨ã‚µãƒ ãƒã‚¤ãƒ«URLã®çµ„ã¿åˆã‚ã›ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if thumbnail_url:
            self.cache[url] = thumbnail_url
            self._save_cache()
    
    def get_multiple(self, urls):
        """è¤‡æ•°ã®URLã«å¯¾ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ä¸€æ‹¬å–å¾—"""
        result = {}
        for url in urls:
            cached_thumbnail = self.get(url)
            if cached_thumbnail:
                result[url] = cached_thumbnail
        return result
    
    def set_multiple(self, url_thumbnail_pairs):
        """è¤‡æ•°ã®URL-ã‚µãƒ ãƒã‚¤ãƒ«ãƒšã‚¢ã‚’ä¸€æ‹¬ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        updated = False
        for url, thumbnail_url in url_thumbnail_pairs.items():
            if thumbnail_url and url not in self.cache:
                self.cache[url] = thumbnail_url
                updated = True
        
        if updated:
            self._save_cache()


# æ—¢å­˜ã®é–¢æ•°ã‚’æ–°ã—ã„ã‚¯ãƒ©ã‚¹æ§‹é€ ã§ãƒ©ãƒƒãƒ—ï¼ˆå¾Œæ–¹äº’æ›æ€§ç¶­æŒï¼‰

def generate_html(all_entries, feed_info, date_str, thumbnails=None):
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®HTMLã‚’ç”Ÿæˆï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    html_content, _ = _archive_generator.generate_main_page(
        all_entries, feed_info, date_str, thumbnails
    )
    return html_content


def generate_markdown(all_entries, feed_info, date_str):
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®Markdownã‚’ç”Ÿæˆï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    _, markdown_content = _archive_generator.generate_main_page(
        all_entries, feed_info, date_str
    )
    return markdown_content


def generate_archive_markdown(all_entries, feed_info, date_str):
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç”¨ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    date_obj = datetime.datetime.fromisoformat(date_str)
    _, markdown_content = _archive_generator.generate_daily_archive(
        all_entries, feed_info, date_obj
    )
    return markdown_content


def generate_archive_html(all_entries, feed_info, date_str, thumbnails=None):
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç”¨ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    date_obj = datetime.datetime.fromisoformat(date_str)
    html_content, _ = _archive_generator.generate_daily_archive(
        all_entries, feed_info, date_obj, thumbnails
    )
    return html_content


def save_to_archive(all_entries, feed_info, date_obj, thumbnails=None):
    """æ—¥ä»˜åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    return _archive_generator.save_daily_archive(
        all_entries, feed_info, date_obj, thumbnails
    )


def update_archive_index():
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å…¨ä½“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    _index_generator.update_all_indexes()


def update_yearly_index(year):
    """å¹´åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    yearly_content = _index_generator.generate_yearly_index(year)
    yearly_path = _path_config.get_archive_dir_path(year) + "/index.md"
    
    Path(yearly_path).parent.mkdir(parents=True, exist_ok=True)
    with open(yearly_path, 'w', encoding='utf-8') as f:
        f.write(yearly_content)
    
    print(f"Updated yearly index: {yearly_path}")


def update_monthly_index(year, month):
    """æœˆåˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    monthly_content = _index_generator.generate_monthly_index(year, month)
    monthly_path = _path_config.get_archive_dir_path(year, month) + "/index.md"
    
    Path(monthly_path).parent.mkdir(parents=True, exist_ok=True)
    with open(monthly_path, 'w', encoding='utf-8') as f:
        f.write(monthly_content)
    
    print(f"Updated monthly index: {monthly_path}")


def generate_missing_html_archives():
    """æ—¢å­˜Markdownã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®HTMLç‰ˆã‚’ç”Ÿæˆï¼ˆæ—¢å­˜APIäº’æ›ï¼‰"""
    archives_base = Path("archives")
    if not archives_base.exists():
        print("Archives directory not found.")
        return
    
    converted_count = 0
    
    for year_dir in archives_base.iterdir():
        if not year_dir.is_dir() or not year_dir.name.isdigit():
            continue
        
        for month_dir in year_dir.iterdir():
            if not month_dir.is_dir() or not month_dir.name.isdigit():
                continue
            
            for md_file in month_dir.glob("*.md"):
                if md_file.name == "index.md":
                    continue
                
                html_file = md_file.with_suffix(".html")
                if html_file.exists():
                    continue
                
                try:
                    # æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦å¤‰æ›
                    _archive_generator.convert_markdown_to_html(
                        str(md_file), str(html_file)
                    )
                    converted_count += 1
                except Exception as e:
                    print(f"Error converting {md_file}: {e}")
    
    print(f"Converted {converted_count} markdown files to HTML.")


# æ®‹ã‚Šã®æ—¢å­˜é–¢æ•°ç¾¤ï¼ˆRSSç”Ÿæˆã€URLé‡è¤‡é™¤å»ãªã©ï¼‰ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ãã®ã¾ã¾ä¿æŒ
# ã“ã‚Œã‚‰ã¯å…ƒã®fetch_news.pyã‹ã‚‰ç§»æ¤

def remove_url_duplicates(all_entries):
    """URLé‡è¤‡ã‚’é™¤å»ã™ã‚‹é–¢æ•°"""
    seen_urls = set()
    deduplicated_entries = {}
    
    # å„ªå…ˆãƒ•ã‚£ãƒ¼ãƒ‰ã®é †åºã‚’å®šç¾©
    priority_feeds = ["Tech Blog Weekly", "Zenn", "Qiita", "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆäººæ°—ï¼‰"]
    
    # å„ªå…ˆãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å…ˆã«å‡¦ç†
    for feed_name in priority_feeds:
        if feed_name in all_entries:
            unique_entries = []
            for entry in all_entries[feed_name]:
                if entry.link not in seen_urls:
                    unique_entries.append(entry)
                    seen_urls.add(entry.link)
            deduplicated_entries[feed_name] = unique_entries
    
    # æ®‹ã‚Šã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å‡¦ç†
    for feed_name, entries in all_entries.items():
        if feed_name not in priority_feeds:
            unique_entries = []
            for entry in entries:
                if entry.link not in seen_urls:
                    unique_entries.append(entry)
                    seen_urls.add(entry.link)
            deduplicated_entries[feed_name] = unique_entries
    
    return deduplicated_entries


def get_thumbnail_url(url, session=None, timeout=10):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®URLã‚’å–å¾—ã™ã‚‹"""
    if session is None:
        session = requests.Session()
    
    try:
        response = session.get(url, timeout=timeout, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # OGPç”»åƒã‚’å„ªå…ˆçš„ã«æ¢ã™
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return og_image['content']
        
        # Twitter Cardã®ç”»åƒã‚’æ¢ã™
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            return twitter_image['content']
        
        # ä¸€èˆ¬çš„ãªmetaã‚¿ã‚°ã®ç”»åƒã‚’æ¢ã™
        meta_image = soup.find('meta', attrs={'name': 'image'})
        if meta_image and meta_image.get('content'):
            return meta_image['content']
        
        return None
        
    except Exception as e:
        print(f"Error getting thumbnail for {url}: {e}")
        return None


def fetch_thumbnails_batch(urls, cache, max_workers=5):
    """è¤‡æ•°ã®URLã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä¸¦è¡Œå–å¾—"""
    if not urls:
        return {}
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ—¢å­˜ã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’å–å¾—
    cached_thumbnails = cache.get_multiple(urls)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„URLã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
    urls_to_fetch = [url for url in urls if url not in cached_thumbnails]
    
    if not urls_to_fetch:
        print("All thumbnails found in cache.")
        return cached_thumbnails
    
    print(f"Fetching thumbnails for {len(urls_to_fetch)} URLs...")
    
    new_thumbnails = {}
    with requests.Session() as session:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(get_thumbnail_url, url, session): url 
                for url in urls_to_fetch
            }
            
            for future in concurrent.futures.as_completed(future_to_url, timeout=60):
                url = future_to_url[future]
                try:
                    thumbnail_url = future.result()
                    if thumbnail_url:
                        new_thumbnails[url] = thumbnail_url
                except Exception as e:
                    print(f"Error fetching thumbnail for {url}: {e}")
    
    # æ–°ã—ãå–å¾—ã—ãŸã‚µãƒ ãƒã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if new_thumbnails:
        cache.set_multiple(new_thumbnails)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚‚ã®ã¨æ–°ã—ãå–å¾—ã—ãŸã‚‚ã®ã‚’çµåˆ
    all_thumbnails = {**cached_thumbnails, **new_thumbnails}
    
    print(f"Successfully fetched {len(new_thumbnails)} new thumbnails. "
          f"Total: {len(all_thumbnails)} thumbnails available.")
    
    return all_thumbnails


# ä»–ã®æ—¢å­˜é–¢æ•°ï¼ˆRSSç”Ÿæˆãªã©ï¼‰ã‚‚åŒæ§˜ã«ä¿æŒ...
# ã“ã“ã§ã¯çœç•¥ã—ã¾ã™ãŒã€å®Ÿéš›ã®å®Ÿè£…ã§ã¯å…¨ã¦ã®æ—¢å­˜é–¢æ•°ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ—¢å­˜ã®mainé–¢æ•°ã¨åŒã˜ï¼‰"""
    # æ—¢å­˜ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…
    # æ–°ã—ã„ã‚¯ãƒ©ã‚¹æ§‹é€ ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«æ›´æ–°
    pass


if __name__ == "__main__":
    main()