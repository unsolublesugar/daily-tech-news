import feedparser
import datetime
import os
from pathlib import Path
from xml.dom import minidom
import xml.etree.ElementTree as ET
import requests
from bs4 import BeautifulSoup
import time
import re
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures
import json
import hashlib

# å–å¾—ã™ã‚‹RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆãƒ•ã‚¡ãƒ“ã‚³ãƒ³ä»˜ãï¼‰
FEEDS = {
    "Tech Blog Weekly": {
        "url": "https://yamadashy.github.io/tech-blog-rss-feed/feeds/rss.xml",
        "favicon": "ğŸ’»"
    },
    "Zenn": {
        "url": "https://zenn.dev/feed",
        "favicon": "https://zenn.dev/favicon.ico"
    },
    "Qiita": {
        "url": "https://qiita.com/popular-items/feed", 
        "favicon": "https://cdn.qiita.com/assets/favicons/public/production-c620d3e403342b1022967ba5e3db1aaa.ico"
    },
    "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆäººæ°—ï¼‰": {
        "url": "http://b.hatena.ne.jp/hotentry/it.rss",
        "favicon": "https://b.hatena.ne.jp/favicon.ico"
    },
    "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆæ–°ç€ï¼‰": {
        "url": "https://b.hatena.ne.jp/entrylist/it.rss",
        "favicon": "https://b.hatena.ne.jp/favicon.ico"
    },
    "DevelopersIO": {
        "url": "https://dev.classmethod.jp/feed/",
        "favicon": "https://dev.classmethod.jp/favicon.ico"
    },
    "gihyo.jp": {
        "url": "https://gihyo.jp/dev/feed/rss2",
        "favicon": "https://gihyo.jp/favicon.ico"
    },
    "Publickey": {
        "url": "https://www.publickey1.jp/atom.xml",
        "favicon": "https://www.publickey1.jp/favicon.ico"
    },
    "CodeZine": {
        "url": "https://codezine.jp/rss/new/20/index.xml",
        "favicon": "https://codezine.jp/favicon.ico"
    },
    "InfoQ Japan": {
        "url": "https://feed.infoq.com/jp",
        "favicon": "https://www.infoq.com/favicon.ico"
    },
    "connpass - ã‚¤ãƒ™ãƒ³ãƒˆ": {
        "url": "https://connpass.com/explore/ja.atom",
        "favicon": "https://connpass.com/favicon.ico"
    },
    "TECH PLAY - ã‚¤ãƒ™ãƒ³ãƒˆ": {
        "url": "https://rss.techplay.jp/event/w3c-rss-format/rss.xml",
        "favicon": "https://techplay.jp/favicon.ico"
    },
    "O'Reilly Japan - è¿‘åˆŠ": {
        "url": "https://www.oreilly.co.jp/catalog/soon.xml",
        "favicon": "https://www.oreilly.co.jp/favicon.ico"
    }
}

# å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹ã®ä»¶æ•°
MAX_ENTRIES = 5

class ThumbnailCache:
    """ã‚µãƒ ãƒã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_file_path="thumbnail_cache.json"):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        
        Args:
            cache_file_path (str): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.cache_file_path = cache_file_path
        self.cache = self._load_cache()
    
    def _load_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if os.path.exists(self.cache_file_path):
                with open(self.cache_file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"Warning: Failed to load thumbnail cache: {e}")
        
        return {}
    
    def _save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
        try:
            with open(self.cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Warning: Failed to save thumbnail cache: {e}")
    
    def _get_url_hash(self, url):
        """URLã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ã™ã‚‹"""
        return hashlib.md5(url.encode('utf-8')).hexdigest()
    
    def get(self, url):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’å–å¾—
        
        Args:
            url (str): è¨˜äº‹URL
            
        Returns:
            str or None: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚µãƒ ãƒã‚¤ãƒ«URLã€ã¾ãŸã¯None
        """
        url_hash = self._get_url_hash(url)
        cache_entry = self.cache.get(url_hash)
        
        if cache_entry:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªãŒ7æ—¥ä»¥å†…ãªã‚‰æœ‰åŠ¹ã¨ã™ã‚‹
            import datetime
            cache_time = datetime.datetime.fromisoformat(cache_entry['timestamp'])
            now = datetime.datetime.now()
            
            if (now - cache_time).days < 7:
                return cache_entry.get('thumbnail_url')
        
        return None
    
    def set(self, url, thumbnail_url):
        """
        ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            url (str): è¨˜äº‹URL
            thumbnail_url (str or None): ã‚µãƒ ãƒã‚¤ãƒ«URL
        """
        url_hash = self._get_url_hash(url)
        import datetime
        
        self.cache[url_hash] = {
            'url': url,
            'thumbnail_url': thumbnail_url,
            'timestamp': datetime.datetime.now().isoformat()
        }
    
    def save(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        self._save_cache()
    
    def cleanup_old_entries(self, days_threshold=30):
        """
        å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        
        Args:
            days_threshold (int): å‰Šé™¤å¯¾è±¡ã¨ãªã‚‹æ—¥æ•°ã®é–¾å€¤
        """
        import datetime
        now = datetime.datetime.now()
        
        keys_to_remove = []
        for key, entry in self.cache.items():
            try:
                cache_time = datetime.datetime.fromisoformat(entry['timestamp'])
                if (now - cache_time).days > days_threshold:
                    keys_to_remove.append(key)
            except Exception:
                # ä¸æ­£ãªã‚¨ãƒ³ãƒˆãƒªã¯å‰Šé™¤å¯¾è±¡ã«
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            del self.cache[key]
        
        if keys_to_remove:
            print(f"Cleaned up {len(keys_to_remove)} old cache entries")

def filter_hatena_anonymous_entries(entries):
    """ã¯ã¦ãªåŒ¿åãƒ€ã‚¤ã‚¢ãƒªãƒ¼ã®è¨˜äº‹ã‚’é™¤å¤–ã™ã‚‹"""
    filtered_entries = []
    excluded_count = 0
    
    for entry in entries:
        # ãƒªãƒ³ã‚¯URLãŒã¯ã¦ãªåŒ¿åãƒ€ã‚¤ã‚¢ãƒªãƒ¼ã‹ãƒã‚§ãƒƒã‚¯
        if hasattr(entry, 'link') and 'anond.hatelabo.jp' in entry.link:
            excluded_count += 1
            continue
        filtered_entries.append(entry)
    
    if excluded_count > 0:
        print(f"Excluded {excluded_count} hatena anonymous diary entries")
    
    return filtered_entries

def fetch_feed_entries(feed_url):
    """æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å–å¾—ã™ã‚‹"""
    try:
        feed = feedparser.parse(feed_url)
        return feed.entries
    except Exception as e:
        print(f"Error fetching feed from {feed_url}: {e}")
        return []

def get_article_thumbnail(url, max_retries=2):
    """è¨˜äº‹URLã‹ã‚‰ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒURLã‚’å–å¾—ã™ã‚‹"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    def validate_image_url(img_url):
        """ç”»åƒURLãŒæœ‰åŠ¹ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯"""
        if not img_url or len(img_url) > 2000:  # URLãŒé•·ã™ãã‚‹å ´åˆã¯é™¤å¤–
            return False
        if not img_url.startswith(('http://', 'https://')):
            return False
        # ç”»åƒå½¢å¼ã®ãƒã‚§ãƒƒã‚¯
        if any(ext in img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']):
            return True
        # å‹•çš„ç”Ÿæˆç”»åƒã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆqiitaã€zennãªã©ï¼‰
        if any(domain in img_url for domain in ['qiita-user-contents.imgix.net', 'res.cloudinary.com', 'cdn.image.st-hatena.com']):
            return True
        return False
    
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Open Graphç”»åƒã‚’å„ªå…ˆçš„ã«å–å¾—
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                img_url = og_image['content']
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    from urllib.parse import urljoin
                    img_url = urljoin(url, img_url)
                if validate_image_url(img_url):
                    return img_url
            
            # Twitter Cardç”»åƒ
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                img_url = twitter_image['content']
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    from urllib.parse import urljoin
                    img_url = urljoin(url, img_url)
                if validate_image_url(img_url):
                    return img_url
            
            # è¨˜äº‹å†…æœ€åˆã®ç”»åƒ
            article_img = soup.find('img')
            if article_img and article_img.get('src'):
                img_url = article_img['src']
                if img_url.startswith('//'):
                    img_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    from urllib.parse import urljoin
                    img_url = urljoin(url, img_url)
                if validate_image_url(img_url):
                    return img_url
                
        except Exception as e:
            print(f"Attempt {attempt + 1} failed for {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(1)  # ãƒªãƒˆãƒ©ã‚¤å‰ã«å°‘ã—å¾…æ©Ÿ
            continue
    
    return None  # ç”»åƒãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ

def deduplicate_events(entries, target_count=10):
    """ã‚¤ãƒ™ãƒ³ãƒˆç³»ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®é‡è¤‡ã‚’é™¤å»ï¼ˆã‚·ãƒªãƒ¼ã‚ºç•ªå·é•ã„ã‚’çµ±åˆï¼‰ã—ã€ç›®æ¨™ä»¶æ•°ã‚’ç¢ºä¿"""
    if not entries:
        return entries
    
    # ã‚¤ãƒ™ãƒ³ãƒˆåã®åŸºåº•éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = [
        r'^(.+?)\s*#\d+.*$',  # "æœæ´»ã‚‚ãã‚‚ãä¼š #19" -> "æœæ´»ã‚‚ãã‚‚ãä¼š"
        r'^(.+?)\s*ç¬¬\d+å›.*$',  # "ç¬¬5å›å‹‰å¼·ä¼š" -> "å‹‰å¼·ä¼š" 
        r'^(.+?)\s*Vol\.\d+.*$',  # "å‹‰å¼·ä¼š Vol.3" -> "å‹‰å¼·ä¼š"
        r'^(.+?)\s*\(\d+\).*$',  # "å‹‰å¼·ä¼š(3)" -> "å‹‰å¼·ä¼š"
        r'^(.+?)\s*-\s*\d+.*$',  # "å‹‰å¼·ä¼š - 5" -> "å‹‰å¼·ä¼š"
    ]
    
    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’åŸºåº•åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    event_groups = {}
    
    for entry in entries:
        title = entry.title.strip()
        base_name = title
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§åŸºåº•åã‚’æŠ½å‡º
        for pattern in patterns:
            match = re.match(pattern, title)
            if match:
                base_name = match.group(1).strip()
                break
        
        # åŸºåº•åã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆæœ€æ–°ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’å„ªå…ˆï¼‰
        if base_name not in event_groups:
            event_groups[base_name] = entry
        else:
            # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚ˆã‚Šæ–°ã—ã„å ´åˆã¯ç½®ãæ›ãˆ
            existing_date = getattr(event_groups[base_name], 'published_parsed', None)
            current_date = getattr(entry, 'published_parsed', None)
            
            if current_date and existing_date:
                if current_date > existing_date:
                    event_groups[base_name] = entry
            elif current_date and not existing_date:
                event_groups[base_name] = entry
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’è¿”ã™ï¼ˆå…ƒã®é †åºã‚’ä¿æŒï¼‰
    deduplicated = []
    seen_bases = set()
    
    for entry in entries:
        title = entry.title.strip()
        base_name = title
        
        for pattern in patterns:
            match = re.match(pattern, title)
            if match:
                base_name = match.group(1).strip()
                break
        
        if base_name not in seen_bases:
            deduplicated.append(event_groups[base_name])
            seen_bases.add(base_name)
            
            # ç›®æ¨™ä»¶æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
            if len(deduplicated) >= target_count:
                break
    
    return deduplicated

def deduplicate_urls_across_feeds(all_entries):
    """ãƒ•ã‚£ãƒ¼ãƒ‰é–“ã§ã®URLé‡è¤‡ã‚’é™¤å»ã—ã€è£œå¡«ã‚’è¡Œã†"""
    seen_urls = set()
    deduplicated_feeds = {}
    
    for feed_name, entries in all_entries.items():
        if not entries:
            deduplicated_feeds[feed_name] = entries
            continue
            
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã§ç›®æ¨™ä»¶æ•°ã‚’æ±ºå®š
        target_count = 10 if "ã‚¤ãƒ™ãƒ³ãƒˆ" in feed_name else 5
        
        # URLé‡è¤‡é™¤å»
        unique_entries = []
        for entry in entries:
            if hasattr(entry, 'link') and entry.link not in seen_urls:
                seen_urls.add(entry.link)
                unique_entries.append(entry)
                
                # ç›®æ¨™ä»¶æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
                if len(unique_entries) >= target_count:
                    break
        
        # ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ã‚£ãƒ¼ãƒ‰ã®å ´åˆã¯ã•ã‚‰ã«ã‚¤ãƒ™ãƒ³ãƒˆé‡è¤‡é™¤å»ã‚’é©ç”¨
        if "ã‚¤ãƒ™ãƒ³ãƒˆ" in feed_name:
            unique_entries = deduplicate_events(unique_entries, target_count)
        
        deduplicated_feeds[feed_name] = unique_entries
    
    return deduplicated_feeds

def fetch_all_thumbnails(all_entries, max_workers=10, use_cache=True):
    """å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã®å…¨è¨˜äº‹ã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä¸¦åˆ—å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
    # å…¨è¨˜äº‹ã®URLãƒªã‚¹ãƒˆã‚’ä½œæˆ
    all_urls = []
    for entries in all_entries.values():
        all_urls.extend([entry.link for entry in entries])
    
    print(f"Fetching thumbnails for {len(all_urls)} articles...")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
    cache = ThumbnailCache() if use_cache else None
    thumbnails = {}
    urls_to_fetch = []
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã§ãã‚‹ã‚‚ã®ã¯å…ˆã«å‡¦ç†
    if cache:
        cache_hits = 0
        for url in all_urls:
            cached_thumbnail = cache.get(url)
            if cached_thumbnail is not None:
                thumbnails[url] = cached_thumbnail
                cache_hits += 1
            else:
                urls_to_fetch.append(url)
        
        if cache_hits > 0:
            print(f"Cache hits: {cache_hits}/{len(all_urls)} thumbnails")
    else:
        urls_to_fetch = all_urls
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„URLã®ã¿ä¸¦åˆ—å–å¾—
    if urls_to_fetch:
        print(f"Fetching {len(urls_to_fetch)} new thumbnails in parallel...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æœªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®URLã«å¯¾ã—ã¦ä¸¦åˆ—ã§ã‚µãƒ ãƒã‚¤ãƒ«å–å¾—ã‚’å®Ÿè¡Œ
            future_to_url = {
                executor.submit(get_article_thumbnail, url): url 
                for url in urls_to_fetch
            }
            
            completed = 0
            total = len(urls_to_fetch)
            
            # å®Œäº†ã—ãŸå‡¦ç†ã‹ã‚‰é †æ¬¡çµæœã‚’å–å¾—
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                completed += 1
                
                try:
                    thumbnail_url = future.result(timeout=15)
                    thumbnails[url] = thumbnail_url
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    if cache:
                        cache.set(url, thumbnail_url)
                    
                    print(f"Progress: {completed}/{total} new thumbnails fetched")
                except Exception as e:
                    print(f"Error fetching thumbnail for {url}: {e}")
                    thumbnails[url] = None
                    
                    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆNoneã¨ã—ã¦ï¼‰
                    if cache:
                        cache.set(url, None)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
    if cache:
        cache.cleanup_old_entries()  # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        cache.save()
        print("Thumbnail cache updated")
                
    return thumbnails

def generate_html(all_entries, feed_info, date_str, thumbnails=None):
    """å–å¾—ã—ãŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‹ã‚‰HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹"""
    site_title = f"ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({date_str})"
    site_description = "æ—¥æœ¬ã®ä¸»è¦ãªæŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ã®æœ€æ–°äººæ°—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’æ¯æ—¥ãŠå±Šã‘ã—ã¾ã™ã€‚"
    site_url = "https://unsolublesugar.github.io/daily-tech-news/"
    og_image_url = f"{site_url}assets/images/OGP.png"
    twitter_user = "@unsoluble_sugar"

    html = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{site_title}</title>
    
    <!-- OGP Tags -->
    <meta property="og:title" content="{site_title}">
    <meta property="og:description" content="{site_description}">
    <meta property="og:type" content="website">
    <meta property="og:url" content="{site_url}">
    <meta property="og:image" content="{og_image_url}">
    <meta property="og:site_name" content="ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹">
    
    <!-- Twitter Card Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="{twitter_user}">
    
    <!-- Favicon Tags -->
    <link rel="apple-touch-icon" sizes="180x180" href="assets/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="assets/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="assets/favicons/favicon-16x16.png">
    <link rel="manifest" href="assets/favicons/site.webmanifest">
    <link rel="shortcut icon" href="assets/favicons/favicon.ico">

    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
        }}
        .card {{
            border: 1px solid #e1e5e9;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
            background-color: #f8f9fa;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: box-shadow 0.2s ease;
            text-decoration: none;
            color: inherit;
            display: block;
        }}
        .card:hover {{
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }}
        .card-content {{
            display: flex;
            align-items: flex-start;
            gap: 15px;
        }}
        .card-image {{
            border-radius: 6px;
            object-fit: cover;
            flex-shrink: 0;
        }}
        .card-text {{
            flex: 1;
        }}
        .card-title {{
            margin: 0 0 8px 0;
            font-size: 16px;
            line-height: 1.4;
            color: #0969da;
            font-weight: 600;
        }}
        .card-source {{
            margin: 0;
            font-size: 12px;
            color: #656d76;
        }}
        h1, h2 {{
            color: #1f2328;
        }}
        .rss-info {{
            background: #f6f8fa;
            padding: 16px;
            border-radius: 8px;
            margin: 20px 0;
        }}
        .footer {{
            margin-top: 40px;
            padding: 20px 0;
            border-top: 1px solid #e1e5e9;
            text-align: center;
            font-size: 14px;
            color: #656d76;
        }}
        .footer a {{
            color: #0969da;
            text-decoration: none;
        }}
        .footer a:hover {{
            text-decoration: underline;
        }}
    </style>
</head>
<body>
    <h1>{site_title}</h1>
    
    <p>ğŸ“š <a href="archives/index.html">éå»ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã‚‹</a> | ğŸ“¡ <a href="https://unsolublesugar.github.io/daily-tech-news/rss.xml">RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è³¼èª­</a></p>
    
    <p>æ—¥æœ¬ã®ä¸»è¦ãªæŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ã®æœ€æ–°äººæ°—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚</p>
    
    <div class="rss-info">
        <p>æ¯æ—¥JST 7:00ã«è‡ªå‹•æ›´æ–°</p>
    </div>
    
    <hr>
"""
    
    for feed_name, entries in all_entries.items():
        favicon = feed_info[feed_name]["favicon"]
        if favicon.startswith("http"):
            favicon_display = f'<img src="{favicon}" width="16" height="16" alt="{feed_name}">'
        else:
            favicon_display = favicon
        
        html += f"    <h2>{favicon_display} {feed_name}</h2>\n"
        
        if not entries:
            html += "    <p>è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚</p>\n"
        else:
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¯ã™ã§ã«URLé‡è¤‡é™¤å»æ¸ˆã¿
            for entry in entries:
                title = entry.title
                link = entry.link
                
                # äº‹å‰å–å¾—æ¸ˆã¿ã®ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä½¿ç”¨
                thumbnail_url = thumbnails.get(link) if thumbnails else None
                
                escaped_title = title.replace('"', '&quot;').replace('<', '&lt;').replace('>', '&gt;')
                
                if thumbnail_url:
                    escaped_url = thumbnail_url.replace('"', '&quot;').replace('<', '&lt;').replace('>', '&gt;')
                    card_html = f'    <a href="{link}" class="card">\n        <div class="card-content">\n            <img src="{escaped_url}" width="120" height="90" alt="{escaped_title}" class="card-image">\n            <div class="card-text">\n                <h4 class="card-title">{title}</h4>\n                <p class="card-source">{feed_name}</p>\n            </div>\n        </div>\n    </a>\n'
                else:
                    card_html = f'    <a href="{link}" class="card">\n        <div class="card-content">\n            <div class="card-text">\n                <h4 class="card-title">{title}</h4>\n                <p class="card-source">{feed_name}</p>\n            </div>\n        </div>\n    </a>\n'
                html += card_html
        
        html += "    <hr>\n"
    
    html += """
    <div class="footer">
        <p>ğŸš€ é‹å–¶è€…: <a href="https://x.com/unsoluble_sugar" target="_blank" rel="noopener">@unsoluble_sugar</a> | 
        ğŸ“ <a href="https://github.com/unsolublesugar/daily-tech-news" target="_blank" rel="noopener">GitHub Repository</a></p>
    </div>
</body>
</html>"""
    
    return html

def generate_markdown(all_entries, feed_info, date_str):
    """å–å¾—ã—ãŸã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‹ã‚‰Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹"""
    markdown = f"# ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({date_str})\n\n"
    markdown += """ğŸ“š [éå»ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã‚‹](archives/index.md) | ğŸ¨ [ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºç‰ˆã‚’è¦‹ã‚‹](https://unsolublesugar.github.io/daily-tech-news/)

æ—¥æœ¬ã®ä¸»è¦ãªæŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ã®æœ€æ–°äººæ°—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚

â€»æ¯æ—¥JST 7:00ã«è‡ªå‹•æ›´æ–°

## ğŸ¨ ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºç‰ˆã‚‚ã‚ã‚Šã¾ã™

GitHub Pagesç‰ˆã§ã¯å„è¨˜äº‹ãŒã‚«ãƒ¼ãƒ‰å½¢å¼ã§è¦‹ã‚„ã™ãè¡¨ç¤ºã•ã‚Œã¾ã™ï¼š  
https://unsolublesugar.github.io/daily-tech-news/

---
"""

    for feed_name, entries in all_entries.items():
        favicon = feed_info[feed_name]["favicon"]
        if favicon.startswith("http"):
            # ãƒ•ã‚¡ãƒ“ã‚³ãƒ³URLã®å ´åˆ
            favicon_display = f'<img src="{favicon}" width="16" height="16" alt="{feed_name}">'
        else:
            # çµµæ–‡å­—ã®å ´åˆ
            favicon_display = favicon
        markdown += f"## {favicon_display} {feed_name}\n\n"
        if not entries:
            markdown += "è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        else:
            # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¯ã™ã§ã«URLé‡è¤‡é™¤å»æ¸ˆã¿
            for entry in entries:
                title = entry.title
                link = entry.link
                
                # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒ³ã‚¯å½¢å¼ã§è¡¨ç¤º
                markdown += f"- [{title}]({link})\n"
        
        markdown += "\n\n---\n"
    
    markdown += "## License\n\nThis project is licensed under the [MIT License](LICENSE).\n"
    
    return markdown

def save_to_archive(content, date_obj):
    """æ—¥ä»˜åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    year = date_obj.year
    month = f"{date_obj.month:02d}"
    date_str = date_obj.isoformat()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    archive_dir = Path(f"archives/{year}/{month}")
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸Šæ›¸ãï¼‰
    archive_file = archive_dir / f"{date_str}.md"
    if archive_file.exists():
        print(f"Overwriting existing archive: {archive_file}")
    else:
        print(f"Creating new archive: {archive_file}")
    
    with open(archive_file, "w", encoding="utf-8") as f:
        f.write(content)
    
    return archive_file

def update_monthly_index(year, month):
    """æœˆåˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°"""
    archive_dir = Path(f"archives/{year}/{month:02d}")
    if not archive_dir.exists():
        return
    
    # ãã®æœˆã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    md_files = sorted([f for f in archive_dir.iterdir() if f.suffix == '.md' and f.name != 'index.md'])
    
    # æœˆåˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_content = f"# {year}å¹´{month}æœˆã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n"
    index_content += f"{year}å¹´{month}æœˆã«å–å¾—ã—ãŸãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä¸€è¦§ã§ã™ã€‚\n\n"
    
    for md_file in reversed(md_files):  # æ–°ã—ã„é †
        date_str = md_file.stem
        index_content += f"- [{date_str}]({md_file.name})\n"
    
    index_content += f"\n[â† {year}å¹´ä¸€è¦§ã«æˆ»ã‚‹](../index.md)\n"
    
    with open(archive_dir / "index.md", "w", encoding="utf-8") as f:
        f.write(index_content)

def update_yearly_index(year):
    """å¹´åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°"""
    year_dir = Path(f"archives/{year}")
    if not year_dir.exists():
        return
    
    # ãã®å¹´ã®æœˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å–å¾—
    month_dirs = sorted([d for d in year_dir.iterdir() if d.is_dir() and d.name.isdigit()])
    
    # å¹´åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_content = f"# {year}å¹´ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n"
    index_content += f"{year}å¹´ã«å–å¾—ã—ãŸãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœˆåˆ¥ä¸€è¦§ã§ã™ã€‚\n\n"
    
    for month_dir in reversed(month_dirs):  # æ–°ã—ã„é †
        month = int(month_dir.name)
        index_content += f"- [{year}å¹´{month}æœˆ]({month_dir.name}/index.md)\n"
    
    index_content += f"\n[â† ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ä¸€è¦§ã«æˆ»ã‚‹](../index.md)\n"
    
    with open(year_dir / "index.md", "w", encoding="utf-8") as f:
        f.write(index_content)

def update_archive_index():
    """ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å…¨ä½“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°"""
    archives_dir = Path("archives")
    if not archives_dir.exists():
        return
    
    # å¹´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä¸€è¦§ã‚’å–å¾—
    year_dirs = sorted([d for d in archives_dir.iterdir() if d.is_dir() and d.name.isdigit()])
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    index_content = "# ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–\n\n"
    index_content += "éå»ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å¹´åˆ¥ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã§ã™ã€‚\n\n"
    
    for year_dir in reversed(year_dirs):  # æ–°ã—ã„é †
        year = year_dir.name
        index_content += f"- [{year}å¹´]({year}/index.md)\n"
    
    index_content += f"\n[â† ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«æˆ»ã‚‹](../README.md)\n"
    
    with open(archives_dir / "index.md", "w", encoding="utf-8") as f:
        f.write(index_content)

def update_readme_with_archive_link(content):
    """README.mdã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¨RSSã¸ã®ãƒªãƒ³ã‚¯ã‚’è¿½åŠ ï¼ˆæ—¢ã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ï¼‰"""
    # generate_markdowné–¢æ•°ã§æ—¢ã«ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¨RSSãƒªãƒ³ã‚¯ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€
    # è¿½åŠ å‡¦ç†ã¯ä¸è¦ã€‚ãã®ã¾ã¾è¿”ã™
    return content

def generate_rss_feed(all_entries, feed_info, date_obj):
    """RSS XMLãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""
    # RSSè¦ç´ ã®ä½œæˆ
    rss = ET.Element('rss', version='2.0', attrib={'xmlns:atom': 'http://www.w3.org/2005/Atom'})
    channel = ET.SubElement(rss, 'channel')
    
    # ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±
    ET.SubElement(channel, 'title').text = 'ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹'
    ET.SubElement(channel, 'link').text = 'https://unsolublesugar.github.io/daily-tech-news/'
    ET.SubElement(channel, 'description').text = 'æ—¥æœ¬ã®ä¸»è¦ãªæŠ€è¡“ç³»ãƒ¡ãƒ‡ã‚£ã‚¢ã®æœ€æ–°äººæ°—ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’æ¯æ—¥ãŠå±Šã‘ã—ã¾ã™'
    ET.SubElement(channel, 'language').text = 'ja'
    ET.SubElement(channel, 'pubDate').text = date_obj.strftime('%a, %d %b %Y %H:%M:%S +0000')
    ET.SubElement(channel, 'lastBuildDate').text = date_obj.strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    # Atomè‡ªå·±å‚ç…§ãƒªãƒ³ã‚¯
    atom_link = ET.SubElement(channel, 'atom:link')
    atom_link.set('href', 'https://unsolublesugar.github.io/daily-tech-news/rss.xml')
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ 
    for feed_name, entries in all_entries.items():
        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã¯ã™ã§ã«URLé‡è¤‡é™¤å»æ¸ˆã¿
        for entry in entries:
            item = ET.SubElement(channel, 'item')
            # RSSã‚¿ã‚¤ãƒˆãƒ«ã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆHTMLã‚¿ã‚°ã‚„çµµæ–‡å­—ã‚’é™¤å»ï¼‰
            clean_title = re.sub(r'<[^>]+>', '', entry.title)  # HTMLã‚¿ã‚°ã‚’é™¤å»
            ET.SubElement(item, 'title').text = clean_title
            ET.SubElement(item, 'link').text = entry.link
            ET.SubElement(item, 'description').text = f'{feed_name}ã‹ã‚‰ã®è¨˜äº‹: {entry.title}'
            ET.SubElement(item, 'guid').text = entry.link
            
            # å…¬é–‹æ—¥ï¼ˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã«æ—¥ä»˜ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°ä»Šæ—¥ï¼‰
            pub_date = getattr(entry, 'published_parsed', None)
            if pub_date:
                pub_datetime = datetime.datetime(*pub_date[:6])
                ET.SubElement(item, 'pubDate').text = pub_datetime.strftime('%a, %d %b %Y %H:%M:%S +0000')
            else:
                ET.SubElement(item, 'pubDate').text = date_obj.strftime('%a, %d %b %Y %H:%M:%S +0000')
    
    return rss

def save_rss_feed(rss_element):
    """RSS XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    # XMLã‚’æ•´å½¢ã—ã¦ä¿å­˜
    rough_string = ET.tostring(rss_element, encoding='unicode')
    reparsed = minidom.parseString(rough_string)
    pretty_xml = reparsed.toprettyxml(indent="  ", encoding='utf-8')
    
    with open("rss.xml", "wb") as f:
        f.write(pretty_xml)
    
    print("RSS feed generated: rss.xml")

def generate_slack_message(all_entries, date):
    """Slacké€šçŸ¥ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
    # æ³¨ç›®è¨˜äº‹ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰1-2ä»¶ï¼‰
    featured_articles = []
    
    # å„ªå…ˆåº¦ã®é«˜ã„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’é¸æŠ
    priority_feeds = ["Tech Blog Weekly", "Zenn", "Qiita", "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆäººæ°—ï¼‰"]
    
    for feed_name in priority_feeds:
        if feed_name in all_entries and all_entries[feed_name]:
            # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€å¤§2ä»¶å–å¾—
            for entry in all_entries[feed_name][:2]:
                if len(featured_articles) < 6:  # æœ€å¤§6ä»¶ã¾ã§
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰HTMLã‚¿ã‚°ã‚’é™¤å»
                    clean_title = re.sub(r'<[^>]+>', '', entry.title)
                    featured_articles.append({
                        "title": clean_title,
                        "link": entry.link
                    })
    
    # ç·è¨˜äº‹æ•°ã‚’è¨ˆç®—
    total_articles = sum(len(entries) for entries in all_entries.values())
    
    # Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
    featured_text = "\n".join([
        f"â€¢ <{article['link']}|{article['title']}>"
        for article in featured_articles
    ])
    
    slack_payload = {
        "text": f"ğŸ“° ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({date.isoformat()})",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"ğŸ“° ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({date.isoformat()})"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"ğŸ”¥ *æ³¨ç›®è¨˜äº‹*\n{featured_text}"
                }
            },
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"ğŸ“Š *æ›´æ–°ã‚µãƒãƒªãƒ¼*: {total_articles}è¨˜äº‹ã‚’æ›´æ–°\n\nğŸ”— <https://unsolublesugar.github.io/daily-tech-news/|ã‚«ãƒ¼ãƒ‰è¡¨ç¤ºç‰ˆã‚’è¦‹ã‚‹>\nğŸ“° <https://github.com/unsolublesugar/daily-tech-news|GitHub ãƒªãƒã‚¸ãƒˆãƒª>"
                }
            },
            {
                "type": "context",
                "elements": [
                    {
                        "type": "mrkdwn",
                        "text": "âš¡ GitHub Actions ã§è‡ªå‹•æ›´æ–° | ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã§é«˜é€ŸåŒ–"
                    }
                ]
            }
        ]
    }
    
    return slack_payload

def save_slack_message(slack_payload):
    """Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    with open("slack_message.json", "w", encoding="utf-8") as f:
        json.dump(slack_payload, f, ensure_ascii=False, indent=2)
    print("Slack message generated: slack_message.json")

if __name__ == "__main__":
    script_start_time = time.time()
    # JSTï¼ˆæ—¥æœ¬æ™‚é–“ï¼‰åŸºæº–ã§æ—¥ä»˜ã‚’å–å¾—
    jst = datetime.timezone(datetime.timedelta(hours=9))
    today = datetime.datetime.now(jst).date()
    
    all_entries = {}
    for name, feed_info in FEEDS.items():
        print(f"Fetching entries from {name}...")
        entries = fetch_feed_entries(feed_info["url"])
        
        # ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ã¯ã¦ãªåŒ¿åãƒ€ã‚¤ã‚¢ãƒªãƒ¼ã‚’é™¤å¤–
        if name in ["ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆäººæ°—ï¼‰", "ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ - ITï¼ˆæ–°ç€ï¼‰"]:
            entries = filter_hatena_anonymous_entries(entries)
        
        all_entries[name] = entries
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰é–“URLé‡è¤‡é™¤å»ã¨è£œå¡«
    print("Removing duplicate URLs across feeds...")
    all_entries = deduplicate_urls_across_feeds(all_entries)
    
    # ğŸš€ å…¨ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä¸¦åˆ—å–å¾—ï¼ˆå¤§å¹…é«˜é€ŸåŒ–ï¼‰
    start_time = time.time()
    thumbnails = fetch_all_thumbnails(all_entries)
    thumbnail_time = time.time() - start_time
    print(f"Thumbnail fetching completed in {thumbnail_time:.2f} seconds")
    
    # Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
    markdown_content = generate_markdown(all_entries, FEEDS, today.isoformat())
    
    # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆï¼ˆäº‹å‰å–å¾—æ¸ˆã¿ã‚µãƒ ãƒã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼‰
    html_content = generate_html(all_entries, FEEDS, today.isoformat(), thumbnails)
    
    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ä¿å­˜
    archive_file = save_to_archive(markdown_content, today)
    print(f"Archived to: {archive_file}")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸æ›´æ–°
    update_monthly_index(today.year, today.month)
    update_yearly_index(today.year)
    update_archive_index()
    
    # README.mdæ›´æ–°ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒªãƒ³ã‚¯ä»˜ãï¼‰
    readme_content = update_readme_with_archive_link(markdown_content)
    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    # index.htmlç”Ÿæˆï¼ˆã‚«ãƒ¼ãƒ‰è¡¨ç¤ºç”¨ï¼‰
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_content)
    print("Generated index.html with card layout")
    
    # RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆ
    rss_feed = generate_rss_feed(all_entries, FEEDS, today)
    save_rss_feed(rss_feed)
    
    # Slackãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
    slack_message = generate_slack_message(all_entries, today)
    save_slack_message(slack_message)
        
    total_time = time.time() - script_start_time
    print(f"Successfully updated README.md, index.html, archive structure, and RSS feed.")
    print(f"Total execution time: {total_time:.2f} seconds (thumbnail fetching: {thumbnail_time:.2f}s)")